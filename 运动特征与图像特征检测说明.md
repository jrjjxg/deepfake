# Deepfake检测非深度学习方法分析与开发报告

## 一、现有代码分析：基于图像特征的检测方法
你提供的 `tools/图像特征训练.py` 和 `推理.py` 实现了一套完整的**传统机器学习（Non-Deep Learning）**检测方案。该方案符合课题要求中“基于图像特征的检测方法”。

### 1.1 核心算法原理
该方法属于**手工特征工程 + 传统分类器**的范式，不依赖神经网络提取特征，而是利用计算机视觉（CV）中的经典图像处理算子。

**特征提取流水线 (`extract_combined_features`)**:
1.  **SRM (Spatial Rich Models) 频域特征**:
    *   **原理**: 使用特定的高通滤波器（High-pass filters）提取图像残差。
    *   **目的**: 捕捉 GAN 生成图像时引入的微弱高频噪声（Upsampling artifacts）。
    *   **实现**: 使用 3 个 5x5 卷积核对图像滤波，统计滤波后图像的均值、方差、偏度、峰度以及直方图分布。
2.  **LBP (Local Binary Patterns) 纹理特征**:
    *   **原理**: 描述图像局部的纹理模式。
    *   **目的**: 检测伪造人脸皮肤纹理的过于平滑或不自然的噪点。
3.  **GLCM (灰度共生矩阵)**:
    *   **原理**: 统计像素对的联合概率分布。
    *   **特征**: 对比度 (Contrast)、能量 (Energy)、相关性 (Correlation) 等。
4.  **HOG (方向梯度直方图)**:
    *   **原理**: 统计图像梯度的方向和强度。
    *   **目的**: 捕捉人脸五官边缘的异常形状。
5.  **色彩一致性**:
    *   **原理**: 比较人脸中心区域与边缘背景区域的色彩均值差异。
    *   **目的**: 针对早期的 Deepfake（如 FaceSwap），检测由于拼接（Blending）不完美导致的色差。

### 1.2 分类器设计
*   **模型**: **Linear SVM (支持向量机)**。
*   **预处理**: 使用 `StandardScaler` 进行特征标准化（去均值、方差归一化），这对 SVM 的收敛至关重要。
*   **训练策略**: 采用 `class_weight='balanced'` 解决正负样本不平衡问题。

### 1.3 代码评价
*   **优点**: 解释性强，计算资源消耗极低（不需要 GPU），对特定类型的伪造（如且边缘明显的 FaceSwap）效果不错。
*   **缺点**: 对现代高保真 Deepfake（如 GHOST, SimSwap）的检测能力可能不如深度学习模型；泛化能力依赖于特征设计的完备性。

---

## 二、新功能开发初步思路：基于脸部运动特征的检测方法
根据课题要求，你需要实现“基于脸部运动特征的检测方法”。这类方法通常基于**时间序列分析**，寻找视频帧之间的不自然跳动或生理特征异常。

以下是两个推荐的可行方案，均不依赖庞大的深度学习黑盒模型，符合“特征工程”的定位：

### 方案 A：基于眨眼频率分析 (Eye Aspect Ratio, EAR)
**原理**: 早期或低质量的 Deepfake 模型往往因为缺乏闭眼数据，导致生成的人脸**眨眼频率极低**或**眨眼动作不完整**。

**实现步骤**:
1.  **人脸关键点检测**: 使用 `dlib` (68点) 或 `MediaPipe` (468点) 提取每一帧的人脸关键点。
2.  **计算 EAR (Eye Aspect Ratio)**:
    *   对于每只眼睛，计算纵向距离与横向距离的比值。
    *   公式: $EAR = \frac{||p_2-p_6|| + ||p_3-p_5||}{2||p_1-p_4||}$ （其中 $p_1...p_6$ 为眼部关键点）。
3.  **时序分析**:
    *   绘制 EAR 随时间变化的曲线。
    *   **特征提取**: 
        *   眨眼次数（每分钟眨眼率）。
        *   平均闭眼时长（Deepfake 可能出现长时间不闭眼）。
        *   EAR 曲线的方差（真实眨眼是快速的动作，曲线波动大）。
4.  **判别**: 设定阈值（如眨眼率 < 10次/分）或输入 SVM 分类。

### 方案 B：基于关键点时域抖动分析 (Landmark Temporal Jitter)
**原理**: Deepfake 需要将生成的脸“贴”回原视频。如果对齐（Alignment）或仿射变换（Affine Transformation）处理不好，五官（特别是鼻子、嘴角）在连续帧之间会有**微小的非自然抖动**，而真实视频中人的微表情是平滑的。

**实现步骤**:
1.  **关键点提取**: 对视频所有帧提取面部 68 个关键点。
2.  **全局运动补偿**: 
    *   选取鼻梁点（通常最稳定）作为锚点，将所有帧的人脸对齐，消除头部转动影响。
3.  **计算抖动特征**:
    *   计算嘴部、眉毛等区域关键点在相邻帧之间的**欧氏距离**。
    *   计算距离的变化率（加速度）。
4.  **特征统计**:
    *   统计抖动幅度的均值、方差。
    *   Deepfake 视频往往在高频部分的抖动能量更高。
5.  **判别**: 将这些统计量作为特征向量输入 SVM 或 随机森林。

---

## 三、数据集推荐与说明 (Dataset Recommendations)

针对你计划开发的“运动特征”检测方法，结合你现有的环境，我们推荐以下数据集策略：

### 3.1 眨眼频率检测 (Eye Blinking Detection)
*   **推荐数据集：UADFV (University at Albany DeepFake Video)**
    *   **适用性**：这是专门为“眨眼检测”研究发布的数据集。数据集中的假视频主要由早期算法生成，大多存在**不眨眼**或**眨眼不自然**的显著缺陷。
    *   **预期效果**：使用 EAR 算法在此数据集上极易达到 high-performance (>95% AUC)，非常适合作为**基础演示**和**完成课题指标**。
    *   **状态**：本地已有 (`e:\DeepfakeBench\datasets\rgb\UADFV`)。

### 3.2 关键点时域抖动检测 (Temp oral Jitter)
*   **推荐数据集：Celeb-DF-v2/v1**
    *   **适用性**：相比 UADFV，Celeb-DF 的视觉质量极高，单帧很难看出破绽。但其生成过程中仍存在细微的物理不一致（如快速转头时的五官漂移），是测试**时域抖动算法**的最佳试金石。
    *   **预期效果**：具有挑战性，能体现算法对“高保真”伪造的检测能力。
    *   **状态**：本地已有 (`e:\DeepfakeBench\datasets\rgb\Celeb-DF-v1`)。

### 3.3 进阶研究 (可选)
如果需要进一步提升研究的深度和广度（例如发表高水平论文），建议补充以下数据集：
*   **FaceForensics++ (FF++)**: 工业界和学术界最通用的训练基准，适合训练鲁棒的基础模型。
*   **DFDC (Deepfake Detection Challenge)**: 包含大量极端光照和复杂场景，适合测试泛化性。
*   **Deepfake-Eval-2024**: 用于评估模型在对抗最新（2024年）生成技术时的表现。

---

### 总结
你现在的代码库中已经有了**图像特征**的完美参考实现。接下来的重点是引入 `dlib` 或 `MediaPipe` 库库，利用本地的 **UADFV** 数据集快速跑通**眨眼检测**流程，随后利用 **Celeb-DF** 验证**时域抖动**检测的有效性。
