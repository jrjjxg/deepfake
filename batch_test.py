"""
Batch Testing Script for DeepfakeBench
åŠŸèƒ½ï¼šæ‰¹é‡æµ‹è¯•æ‰€æœ‰æ¨¡å‹åœ¨ Celeb-DF-v1 æ•°æ®é›†ä¸Šçš„è¡¨ç°
ç‰¹æ€§ï¼š
  - è‡ªåŠ¨è·³è¿‡å‡ºé”™çš„æ¨¡å‹
  - æ”¶é›†å¹¶æ±‡æ€»æ‰€æœ‰ç»“æœ
  - ä¿å­˜ç»“æœåˆ° CSV æ–‡ä»¶
"""

import os
import sys
import yaml
import torch
import random
import traceback
import numpy as np
from datetime import datetime
from tqdm import tqdm

# Windows multiprocessing fix - å¿…é¡»åœ¨å…¶ä»–å¯¼å…¥ä¹‹å‰è®¾ç½®
import multiprocessing
if sys.platform == 'win32':
    multiprocessing.set_start_method('spawn', force=True)
import pandas as pd
import torch.backends.cudnn as cudnn
import matplotlib.pyplot as plt
import matplotlib
from matplotlib import rcParams

# å°è¯•å¯¼å…¥ seabornï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨çº¯ matplotlib
try:
    import seaborn as sns
    USE_SEABORN = True
    print("âœ“ Seaborn loaded successfully")
except (ImportError, AttributeError) as e:
    USE_SEABORN = False
    print(f"âš  Seaborn not available ({e}), using matplotlib only")

# è®¾ç½®matplotlibæ”¯æŒä¸­æ–‡æ˜¾ç¤º
try:
    matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS']
except:
    matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial']
    
matplotlib.rcParams['axes.unicode_minus'] = False

# è®¾ç½®æ ·å¼
try:
    if USE_SEABORN:
        plt.style.use('seaborn-v0_8-darkgrid')
        sns.set_palette("husl")
    else:
        plt.style.use('ggplot')
except:
    # å¦‚æœæ²¡æœ‰ seaborn æ ·å¼ï¼Œä½¿ç”¨ç»å…¸æ ·å¼
    try:
        plt.style.use('ggplot')
    except:
        pass  # ä½¿ç”¨é»˜è®¤æ ·å¼

# æ·»åŠ  training ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'training'))

from dataset.abstract_dataset import DeepfakeAbstractBaseDataset
from detectors import DETECTOR
from metrics.utils import get_test_metrics

# ==================== é…ç½® ====================
# æµ‹è¯•æ•°æ®é›† - ä¿®æ”¹è¿™é‡Œæ¥æµ‹è¯•ä¸åŒçš„æ•°æ®é›†
TEST_DATASET = "Celeb-DF-v1"  # å¯é€‰: "Celeb-DF-v1", "UADFV", "CelebDFv2", "FF-DF", etc

# æ¨¡å‹æƒé‡å’Œé…ç½®æ˜ å°„
# å·²ä¿®å¤ï¼šå¯ç”¨æ‰€æœ‰æ¨¡å‹ï¼Œæ·»åŠ äº†OOMä¿æŠ¤
MODELS = {
    # === Naive Models ===
    "xception": {
        "config": "./training/config/detector/xception.yaml",
        "weights": "./training/weights/xception_best.pth",
    },
    "efficientnetb4": {
        "config": "./training/config/detector/efficientnetb4.yaml",
        "weights": "./training/weights/effnb4_best.pth",
    },
    "meso4": {
        "config": "./training/config/detector/meso4.yaml",
        "weights": "./training/weights/meso4_best.pth",
    },
    "meso4Inception": {
        "config": "./training/config/detector/meso4Inception.yaml",
        "weights": "./training/weights/meso4Incep_best.pth",
    },
    # === Spatial Models ===
    "capsule_net": {
        "config": "./training/config/detector/capsule_net.yaml",
        "weights": "./training/weights/capsule_best.pth",
    },
    "ffd": {
        "config": "./training/config/detector/ffd.yaml",
        "weights": "./training/weights/ffd_best.pth",
    },
    "core": {
        "config": "./training/config/detector/core.yaml",
        "weights": "./training/weights/core_best.pth",
    },
    "recce": {
        "config": "./training/config/detector/recce.yaml",
        "weights": "./training/weights/recce_best.pth",
    },
    "ucf": {
        "config": "./training/config/detector/ucf.yaml",
        "weights": "./training/weights/ucf_best.pth",
    },
    # === Frequency Models ===
    "f3net": {
        "config": "./training/config/detector/f3net.yaml",
        "weights": "./training/weights/f3net_best.pth",
    },
    "spsl": {
        "config": "./training/config/detector/spsl.yaml",
        "weights": "./training/weights/spsl_best.pth",
    },
    "srm": {
        "config": "./training/config/detector/srm.yaml",
        "weights": "./training/weights/srm_best.pth",
    },
}

# è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ==================== è¾…åŠ©å‡½æ•° ====================

def init_seed(config):
    """åˆå§‹åŒ–éšæœºç§å­"""
    if config.get('manualSeed') is None:
        config['manualSeed'] = 1024
    random.seed(config['manualSeed'])
    torch.manual_seed(config['manualSeed'])
    if config.get('cuda', True):
        torch.cuda.manual_seed_all(config['manualSeed'])


def load_config(detector_path, test_dataset):
    """åŠ è½½å¹¶åˆå¹¶é…ç½®"""
    with open(detector_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # åŠ è½½æµ‹è¯•é…ç½®
    test_config_path = './training/config/test_config.yaml'
    with open(test_config_path, 'r', encoding='utf-8') as f:
        test_config = yaml.safe_load(f)
    
    # åˆå¹¶é…ç½®
    config.update(test_config)
    config['test_dataset'] = [test_dataset]
    
    # å¼ºåˆ¶è®¾ç½® workers ä¸º 0ï¼Œé¿å… Windows å¤šè¿›ç¨‹å†…å­˜é—®é¢˜
    config['workers'] = 0
    
    # å¤§å¹…å‡å° batch size ä»¥èŠ‚çœæ˜¾å­˜ï¼ˆæ”¹ä¸º 2 ä»¥æ”¯æŒå¤§æ¨¡å‹å¦‚ SRM, RECCEï¼‰
    config['test_batchSize'] = 2
    print(f"  Set batch size to 2 to save GPU memory", flush=True)
    
    # å‡å° frame_num ä»¥èŠ‚çœå†…å­˜
    if 'frame_num' in config:
        config['frame_num'] = {'train': 8, 'test': 8}
    
    return config


def prepare_testing_data(config):
    """å‡†å¤‡æµ‹è¯•æ•°æ®åŠ è½½å™¨"""
    def get_test_data_loader(config, test_name):
        config = config.copy()
        config['test_dataset'] = test_name
        test_set = DeepfakeAbstractBaseDataset(
            config=config,
            mode='test',
        )
        test_data_loader = torch.utils.data.DataLoader(
            dataset=test_set,
            batch_size=config.get('test_batchSize', 32),
            shuffle=False,
            num_workers=int(config.get('workers', 0)),
            collate_fn=test_set.collate_fn,
            drop_last=False
        )
        return test_data_loader, test_set

    test_data_loaders = {}
    test_datasets = {}
    for one_test_name in config['test_dataset']:
        loader, dataset = get_test_data_loader(config, one_test_name)
        test_data_loaders[one_test_name] = loader
        test_datasets[one_test_name] = dataset
    return test_data_loaders, test_datasets


@torch.no_grad()
def inference(model, data_dict):
    """æ¨¡å‹æ¨ç†"""
    predictions = model(data_dict, inference=True)
    return predictions


def test_one_dataset(model, data_loader):
    """æµ‹è¯•å•ä¸ªæ•°æ®é›†"""
    prediction_lists = []
    label_lists = []
    
    for i, data_dict in tqdm(enumerate(data_loader), total=len(data_loader), desc="Testing"):
        # è·å–æ•°æ®
        data, label, mask, landmark = \
            data_dict['image'], data_dict['label'], data_dict['mask'], data_dict['landmark']
        label = torch.where(data_dict['label'] != 0, 1, 0)
        
        # ç§»åŠ¨æ•°æ®åˆ° GPU
        data_dict['image'], data_dict['label'] = data.to(device), label.to(device)
        if mask is not None:
            data_dict['mask'] = mask.to(device)
        if landmark is not None:
            data_dict['landmark'] = landmark.to(device)

        # æ¨¡å‹æ¨ç†
        predictions = inference(model, data_dict)
        label_lists += list(data_dict['label'].cpu().detach().numpy())
        prediction_lists += list(predictions['prob'].cpu().detach().numpy())
    
    return np.array(prediction_lists), np.array(label_lists)


# ==================== å¯è§†åŒ–å‡½æ•° ====================

def create_visualizations(df, test_dataset, timestamp):
    """åˆ›å»ºæ‰€æœ‰å¯è§†åŒ–å›¾è¡¨"""
    output_dir = f"./results_{TEST_DATASET}_{timestamp}"
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"\nğŸ“Š Generating visualizations...")
    
    # 1. AUC å¯¹æ¯”æŸ±çŠ¶å›¾
    create_auc_bar_chart(df, output_dir)
    
    # 2. å¤šæŒ‡æ ‡å¯¹æ¯”å›¾
    create_multi_metric_comparison(df, output_dir)
    
    # 3. æ’åå¯è§†åŒ–
    create_ranking_visualization(df, output_dir)
    
    # 4. ç»¼åˆæ€§èƒ½é›·è¾¾å›¾ï¼ˆå‰5åæ¨¡å‹ï¼‰
    create_radar_chart(df, output_dir)
    
    # 5. è¯¦ç»†å¯¹æ¯”çƒ­åŠ›å›¾
    create_heatmap(df, output_dir)
    
    print(f"âœ“ All visualizations saved to: {output_dir}/")
    return output_dir


def create_auc_bar_chart(df, output_dir):
    """åˆ›å»ºAUCå¯¹æ¯”æŸ±çŠ¶å›¾"""
    if 'auc' not in df.columns or df['auc'].isna().all():
        return
    
    # æŒ‰AUCæ’åº
    df_sorted = df.sort_values('auc', ascending=True)
    
    plt.figure(figsize=(12, 8))
    colors = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(df_sorted)))
    
    bars = plt.barh(df_sorted['model'], df_sorted['auc'], color=colors, edgecolor='black', linewidth=1.5)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (bar, val) in enumerate(zip(bars, df_sorted['auc'])):
        if not np.isnan(val):
            plt.text(val + 0.01, bar.get_y() + bar.get_height()/2, 
                    f'{val:.4f}', va='center', fontsize=10, weight='bold')
    
    plt.xlabel('AUC Score', fontsize=14, weight='bold')
    plt.ylabel('Model', fontsize=14, weight='bold')
    plt.title(f'Model Performance Comparison - AUC on {TEST_DATASET}', 
              fontsize=16, weight='bold', pad=20)
    plt.xlim(0, 1.0)
    plt.grid(axis='x', alpha=0.3, linestyle='--')
    plt.tight_layout()
    
    output_path = os.path.join(output_dir, '01_auc_comparison.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"  âœ“ Saved: {output_path}")


def create_multi_metric_comparison(df, output_dir):
    """åˆ›å»ºå¤šæŒ‡æ ‡å¯¹æ¯”å›¾"""
    metrics = ['auc', 'acc', 'eer', 'ap']
    available_metrics = [m for m in metrics if m in df.columns and not df[m].isna().all()]
    
    if not available_metrics:
        return
    
    # æŒ‰ç¬¬ä¸€ä¸ªå¯ç”¨æŒ‡æ ‡æ’åº
    df_sorted = df.sort_values(available_metrics[0], ascending=False)
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    axes = axes.flatten()
    
    for idx, metric in enumerate(available_metrics):
        ax = axes[idx]
        data = df_sorted.sort_values(metric, ascending=False)
        
        colors = plt.cm.viridis(np.linspace(0.2, 0.9, len(data)))
        bars = ax.bar(range(len(data)), data[metric], color=colors, 
                      edgecolor='black', linewidth=1.5, alpha=0.8)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (bar, val) in enumerate(zip(bars, data[metric])):
            if not np.isnan(val):
                ax.text(i, val + 0.01, f'{val:.3f}', 
                       ha='center', va='bottom', fontsize=9, weight='bold')
        
        ax.set_xlabel('Model', fontsize=12, weight='bold')
        ax.set_ylabel(metric.upper(), fontsize=12, weight='bold')
        ax.set_title(f'{metric.upper()} Comparison', fontsize=14, weight='bold')
        ax.set_xticks(range(len(data)))
        ax.set_xticklabels(data['model'], rotation=45, ha='right', fontsize=9)
        ax.grid(axis='y', alpha=0.3, linestyle='--')
        ax.set_ylim(0, 1.0)
    
    # éšè—å¤šä½™çš„å­å›¾
    for idx in range(len(available_metrics), 4):
        axes[idx].axis('off')
    
    plt.suptitle(f'Multi-Metric Performance Comparison on {TEST_DATASET}', 
                 fontsize=18, weight='bold', y=0.995)
    plt.tight_layout()
    
    output_path = os.path.join(output_dir, '02_multi_metric_comparison.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"  âœ“ Saved: {output_path}")


def create_ranking_visualization(df, output_dir):
    """åˆ›å»ºæ’åå¯è§†åŒ–"""
    if 'auc' not in df.columns or df['auc'].isna().all():
        return
    
    df_sorted = df.sort_values('auc', ascending=False).reset_index(drop=True)
    
    fig, ax = plt.subplots(figsize=(14, 10))
    
    # åˆ›å»ºæ’å
    ranks = np.arange(1, len(df_sorted) + 1)
    scores = df_sorted['auc'].values
    
    # ç»˜åˆ¶æ’åçº¿
    colors = ['gold', 'silver', '#CD7F32']  # é‡‘ã€é“¶ã€é“œ
    for i in range(min(3, len(df_sorted))):
        color = colors[i] if i < 3 else 'skyblue'
        ax.scatter(ranks[i], scores[i], s=800, c=color, edgecolors='black', 
                  linewidth=3, zorder=3, alpha=0.9)
        ax.text(ranks[i], scores[i], f'#{i+1}', ha='center', va='center', 
               fontsize=14, weight='bold', color='black')
    
    # å…¶ä»–æ¨¡å‹
    for i in range(3, len(df_sorted)):
        ax.scatter(ranks[i], scores[i], s=500, c='steelblue', edgecolors='black', 
                  linewidth=2, zorder=3, alpha=0.7)
        ax.text(ranks[i], scores[i], f'#{i+1}', ha='center', va='center', 
               fontsize=11, weight='bold', color='white')
    
    # è¿æ¥çº¿
    ax.plot(ranks, scores, 'o--', color='gray', alpha=0.5, linewidth=2, zorder=1)
    
    # æ·»åŠ æ¨¡å‹åç§°
    for i, (rank, score, model) in enumerate(zip(ranks, scores, df_sorted['model'])):
        offset = 0.02 if i % 2 == 0 else -0.02
        ax.text(rank, score + offset, model, ha='center', va='bottom' if i % 2 == 0 else 'top',
               fontsize=10, weight='bold', bbox=dict(boxstyle='round,pad=0.3', 
               facecolor='white', edgecolor='gray', alpha=0.8))
    
    ax.set_xlabel('Rank', fontsize=14, weight='bold')
    ax.set_ylabel('AUC Score', fontsize=14, weight='bold')
    ax.set_title(f'Model Ranking by AUC on {TEST_DATASET}', 
                fontsize=16, weight='bold', pad=20)
    ax.grid(True, alpha=0.3, linestyle='--')
    ax.set_ylim(min(scores) - 0.05, max(scores) + 0.1)
    
    plt.tight_layout()
    output_path = os.path.join(output_dir, '03_ranking_visualization.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"  âœ“ Saved: {output_path}")


def create_radar_chart(df, output_dir):
    """åˆ›å»ºé›·è¾¾å›¾ï¼ˆå‰5åæ¨¡å‹ï¼‰"""
    metrics = ['auc', 'acc', 'ap']
    available_metrics = [m for m in metrics if m in df.columns and not df[m].isna().all()]
    
    if len(available_metrics) < 2:
        return
    
    # é€‰æ‹©å‰5åæ¨¡å‹
    df_top = df.nlargest(min(5, len(df)), available_metrics[0])
    
    # å‡†å¤‡æ•°æ®
    categories = [m.upper() for m in available_metrics]
    N = len(categories)
    
    # è®¡ç®—è§’åº¦
    angles = [n / float(N) * 2 * np.pi for n in range(N)]
    angles += angles[:1]
    
    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))
    
    colors = plt.cm.Set2(np.linspace(0, 1, len(df_top)))
    
    for idx, (_, row) in enumerate(df_top.iterrows()):
        values = [row[m] for m in available_metrics]
        values += values[:1]
        
        ax.plot(angles, values, 'o-', linewidth=2, label=row['model'], 
               color=colors[idx], markersize=8)
        ax.fill(angles, values, alpha=0.15, color=colors[idx])
    
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(categories, fontsize=12, weight='bold')
    ax.set_ylim(0, 1)
    ax.set_title(f'Top 5 Models - Multi-Metric Radar Chart\n{TEST_DATASET}', 
                fontsize=16, weight='bold', pad=30)
    ax.grid(True, alpha=0.3)
    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=11)
    
    plt.tight_layout()
    output_path = os.path.join(output_dir, '04_radar_chart_top5.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"  âœ“ Saved: {output_path}")


def create_heatmap(df, output_dir):
    """åˆ›å»ºçƒ­åŠ›å›¾"""
    metrics = ['auc', 'acc', 'eer', 'ap']
    available_metrics = [m for m in metrics if m in df.columns and not df[m].isna().all()]
    
    if not available_metrics:
        return
    
    # å‡†å¤‡æ•°æ®
    df_heat = df[['model'] + available_metrics].set_index('model')
    
    # æŒ‰ç¬¬ä¸€ä¸ªæŒ‡æ ‡æ’åº
    df_heat = df_heat.sort_values(available_metrics[0], ascending=False)
    
    plt.figure(figsize=(10, len(df_heat) * 0.5 + 2))
    
    if USE_SEABORN:
        # ä½¿ç”¨ seaborn åˆ›å»ºçƒ­åŠ›å›¾
        sns.heatmap(df_heat.T, annot=True, fmt='.4f', cmap='RdYlGn', 
                   cbar_kws={'label': 'Score'}, linewidths=1.5, linecolor='black',
                   vmin=0, vmax=1, annot_kws={'fontsize': 10, 'weight': 'bold'})
    else:
        # ä½¿ç”¨çº¯ matplotlib åˆ›å»ºçƒ­åŠ›å›¾
        data_array = df_heat.T.values
        im = plt.imshow(data_array, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)
        
        # æ·»åŠ é¢œè‰²æ¡
        cbar = plt.colorbar(im)
        cbar.set_label('Score', fontsize=12, weight='bold')
        
        # è®¾ç½®åˆ»åº¦
        ax = plt.gca()
        ax.set_xticks(np.arange(len(df_heat.index)))
        ax.set_yticks(np.arange(len(df_heat.columns)))
        ax.set_xticklabels(df_heat.index, rotation=45, ha='right', fontsize=10)
        ax.set_yticklabels(df_heat.columns, rotation=0, fontsize=12, weight='bold')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i in range(len(df_heat.columns)):
            for j in range(len(df_heat.index)):
                value = data_array[i, j]
                if not np.isnan(value):
                    text = ax.text(j, i, f'{value:.4f}',
                                 ha="center", va="center", color="black",
                                 fontsize=10, weight='bold')
        
        # æ·»åŠ ç½‘æ ¼çº¿
        ax.set_xticks(np.arange(len(df_heat.index))-0.5, minor=True)
        ax.set_yticks(np.arange(len(df_heat.columns))-0.5, minor=True)
        ax.grid(which="minor", color="black", linestyle='-', linewidth=1.5)
    
    plt.title(f'Performance Heatmap - All Metrics on {TEST_DATASET}', 
             fontsize=16, weight='bold', pad=20)
    plt.xlabel('Model', fontsize=14, weight='bold')
    plt.ylabel('Metric', fontsize=14, weight='bold')
    
    if not USE_SEABORN:
        plt.xticks(rotation=45, ha='right', fontsize=10)
        plt.yticks(rotation=0, fontsize=12, weight='bold')
    
    plt.tight_layout()
    output_path = os.path.join(output_dir, '05_performance_heatmap.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"  âœ“ Saved: {output_path}")





def aggressive_memory_cleanup():
    """å¼ºåŠ›æ¸…ç†GPUå†…å­˜"""
    import gc
    gc.collect()
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        # æ‰“å°å½“å‰GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
        allocated = torch.cuda.memory_allocated() / 1024**2
        reserved = torch.cuda.memory_reserved() / 1024**2
        print(f"  [GPU Memory] Allocated: {allocated:.1f}MB, Reserved: {reserved:.1f}MB")


def test_single_model(model_name, model_info, test_dataset):
    """æµ‹è¯•å•ä¸ªæ¨¡å‹"""
    
    # åœ¨å¼€å§‹æµ‹è¯•å‰å¼ºåŠ›æ¸…ç†GPUå†…å­˜
    print("  Cleaning GPU memory before test...")
    aggressive_memory_cleanup()
    
    print(f"\n{'='*60}")
    print(f"Testing Model: {model_name}")
    print(f"{'='*60}")
    
    try:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(model_info['config']):
            raise FileNotFoundError(f"Config file not found: {model_info['config']}")
        if not os.path.exists(model_info['weights']):
            raise FileNotFoundError(f"Weights file not found: {model_info['weights']}")
        
        # åŠ è½½é…ç½®
        print(f"Loading config from: {model_info['config']}", flush=True)
        config = load_config(model_info['config'], test_dataset)
        
        # åˆå§‹åŒ–ç§å­
        init_seed(config)
        
        # è®¾ç½® cudnn
        if config.get('cudnn', True):
            cudnn.benchmark = True
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        print(f"Preparing test data for {test_dataset}...", flush=True)
        test_data_loaders, test_datasets = prepare_testing_data(config)
        print(f"âœ“ Test data prepared", flush=True)
        
        # å‡†å¤‡æ¨¡å‹
        print(f"Initializing model: {config['model_name']}...", flush=True)
        model_class = DETECTOR[config['model_name']]
        print(f"  Model class loaded: {model_class}", flush=True)
        print(f"  Creating model instance...", flush=True)
        model = model_class(config)
        print(f"  Moving model to device: {device}...", flush=True)
        model = model.to(device)
        print(f"âœ“ Model initialized on {device}", flush=True)
        
        # åŠ è½½æƒé‡
        print(f"Loading weights from: {model_info['weights']}")
        ckpt = torch.load(model_info['weights'], map_location=device)
        
        # æ™ºèƒ½åŠ è½½æƒé‡å‡½æ•°
        def smart_load_state_dict(model, state_dict):
            """è‡ªåŠ¨å¤„ç†å½¢çŠ¶ä¸åŒ¹é…çš„åŠ è½½å‡½æ•°"""
            model_state = model.state_dict()
            filtered_state = {}
            skipped_keys = []
            
            for k, v in state_dict.items():
                if k in model_state:
                    if v.shape == model_state[k].shape:
                        filtered_state[k] = v
                    else:
                        skipped_keys.append(f"{k} (ckpt: {v.shape} vs model: {model_state[k].shape})")
                else:
                    # åŒ…å« unexpected keysï¼Œåæ­£ strict=False ä¼šå¿½ç•¥å®ƒä»¬ï¼Œæˆ–è€…å¦‚æœå¹¸è¿çš„è¯åŒ¹é…ä¸Š
                    filtered_state[k] = v
            
            if skipped_keys:
                print(f"  âš  Skipped {len(skipped_keys)} layers due to shape mismatch:")
                for sk in skipped_keys[:3]:
                    print(f"    - {sk}")
                if len(skipped_keys) > 3: print(f"    - ... and {len(skipped_keys)-3} more")
            
            return model.load_state_dict(filtered_state, strict=False)

        # å°è¯•åŠ è½½æƒé‡
        try:
            model.load_state_dict(ckpt, strict=True)
            print(f"âœ“ Loaded weights (strict mode)")
        except RuntimeError as e:
            print(f"âš  Strict loading failed, trying smart mode (ignoring shape mismatches)...")
            # ä½¿ç”¨æ™ºèƒ½åŠ è½½
            msg = smart_load_state_dict(model, ckpt)
            print(f"âœ“ Loaded weights (smart non-strict mode)")
            if msg.missing_keys:
                print(f"  Missing: {len(msg.missing_keys)} keys")
            if msg.unexpected_keys:
                print(f"  Unexpected: {len(msg.unexpected_keys)} keys")
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        model.eval()
        
        # æµ‹è¯•
        results = {}
        for key in test_data_loaders.keys():
            print(f"\nTesting on {key}...")
            data_dict = test_data_loaders[key].dataset.data_dict
            predictions_nps, label_nps = test_one_dataset(model, test_data_loaders[key])
            
            # è®¡ç®—æŒ‡æ ‡
            metrics = get_test_metrics(
                y_pred=predictions_nps,
                y_true=label_nps,
                img_names=data_dict['image']
            )
            results[key] = metrics
            
            print(f"\n--- Results for {key} ---")
            for k, v in metrics.items():
                print(f"  {k}: {v:.4f}" if isinstance(v, float) else f"  {k}: {v}")
        
        # æ˜¾å¼æ¸…ç†æ¨¡å‹å’Œæ•°æ® - æ›´å½»åº•çš„æ¸…ç†
        print("\n  Cleaning up model and data...")
        try:
            model.cpu()  # å…ˆç§»åŠ¨åˆ°CPU
        except:
            pass
        del model
        del ckpt
        del test_data_loaders
        del test_datasets
        
        # å¼ºåŠ›æ¸…ç†GPUå†…å­˜
        aggressive_memory_cleanup()
        
        return results
        
    except Exception as e:
        # å¦‚æœåœ¨å‡½æ•°å†…éƒ¨å‘ç”Ÿä»»ä½•å¼‚å¸¸ï¼Œé‡æ–°æŠ›å‡ºä»¥ä¾¿å¤–å±‚æ•è·
        print(f"\nâŒ Error in test_single_model: {str(e)}")
        traceback.print_exc()
        
        # å¼ºåŠ›æ¸…ç†å¯èƒ½çš„æ®‹ç•™èµ„æº
        print("  Performing aggressive memory cleanup after error...")
        aggressive_memory_cleanup()
        
        raise


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("DeepfakeBench Batch Testing Script")
    print(f"Test Dataset: {TEST_DATASET}")
    print(f"Device: {device}")
    print(f"Number of Models: {len(MODELS)}")
    print("=" * 70)
    
    # ç»“æœæ”¶é›†
    all_results = []
    successful_models = []
    failed_models = []
    
    # é€ä¸ªæµ‹è¯•æ¨¡å‹
    total_models = len(MODELS)
    for idx, (model_name, model_info) in enumerate(MODELS.items(), 1):
        print(f"\n{'#'*70}")
        print(f"# Progress: {idx}/{total_models} - Testing {model_name}")
        print(f"{'#'*70}")
        
        try:
            results = test_single_model(model_name, model_info, TEST_DATASET)
            
            # æå–æŒ‡æ ‡
            for dataset_name, metrics in results.items():
                result_entry = {
                    "model": model_name,
                    "dataset": dataset_name,
                    "auc": metrics.get('auc', None),
                    "acc": metrics.get('acc', None),
                    "eer": metrics.get('eer', None),
                    "ap": metrics.get('ap', None),
                }
                all_results.append(result_entry)
            
            successful_models.append(model_name)
            print(f"\nâœ“ {model_name}: Test completed successfully!")
            
        except KeyboardInterrupt:
            print(f"\n\nâš ï¸ User interrupted the testing process!")
            print(f"Processed {idx-1}/{total_models} models before interruption.")
            break
            
        except Exception as e:
            print(f"\nâœ— {model_name}: FAILED!")
            print(f"  Error Type: {type(e).__name__}")
            print(f"  Error Message: {str(e)[:300]}")
            
            # åªæ‰“å°æœ€åå‡ è¡Œçš„tracebackï¼Œé¿å…è¿‡é•¿è¾“å‡º
            import sys
            import io
            f = io.StringIO()
            traceback.print_exc(limit=5, file=f)
            error_trace = f.getvalue()
            print(f"\n  Stack Trace (last 5 frames):")
            for line in error_trace.split('\n')[-15:]:
                if line.strip():
                    print(f"    {line}")
            
            failed_models.append({
                "model": model_name,
                "error": f"{type(e).__name__}: {str(e)[:200]}"
            })
            print(f"\n  â­ï¸  Skipping {model_name} and continuing with next model...")
        
        # æ¸…ç† GPU å†…å­˜å’Œ Python åƒåœ¾å›æ”¶
        import gc
        gc.collect()  # Python åƒåœ¾å›æ”¶
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            print(f"  ğŸ§¹ GPU cache cleared", flush=True)
    
    
    # ==================== æ±‡æ€»ç»“æœ ====================
    print("\n")
    print("=" * 70)
    print("BATCH TESTING SUMMARY")
    print("=" * 70)
    
    print(f"\nâœ“ Successful Models ({len(successful_models)}/{len(MODELS)}):")
    for m in successful_models:
        print(f"  - {m}")
    
    if failed_models:
        print(f"\nâœ— Failed Models ({len(failed_models)}/{len(MODELS)}):")
        for m in failed_models:
            print(f"  - {m['model']}: {m['error'][:80]}...")
    
    # ä¿å­˜ç»“æœåˆ° CSV
    if all_results:
        df = pd.DataFrame(all_results)
        
        # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"./batch_test_results_{TEST_DATASET}_{timestamp}.csv"
        df.to_csv(output_file, index=False)
        print(f"\nğŸ“Š Results saved to: {output_file}")
        
        # ==================== æ‰“å°æ’åè¡¨æ ¼ ====================
        print("\n" + "=" * 70)
        print("MODEL RANKING (by AUC)")
        print("=" * 70)
        
        if 'auc' in df.columns:
            df_sorted = df.sort_values('auc', ascending=False).reset_index(drop=True)
            
            # æ‰“å°è¡¨å¤´
            print(f"\n{'Rank':<6} {'Medal':<8} {'Model':<20} {'AUC':<10} {'ACC':<10} {'EER':<10} {'AP':<10}")
            print("-" * 70)
            
            # æ‰“å°æ’å
            medals = ['ğŸ¥‡', 'ğŸ¥ˆ', 'ğŸ¥‰']
            for i, row in df_sorted.iterrows():
                rank = i + 1
                medal = medals[i] if i < 3 else '  '
                model = row['model']
                auc = f"{row['auc']:.4f}" if pd.notna(row['auc']) else 'N/A'
                acc = f"{row['acc']:.4f}" if 'acc' in row and pd.notna(row['acc']) else 'N/A'
                eer = f"{row['eer']:.4f}" if 'eer' in row and pd.notna(row['eer']) else 'N/A'
                ap = f"{row['ap']:.4f}" if 'ap' in row and pd.notna(row['ap']) else 'N/A'
                
                print(f"{rank:<6} {medal:<8} {model:<20} {auc:<10} {acc:<10} {eer:<10} {ap:<10}")
            
            # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
            print("\n" + "=" * 70)
            print("STATISTICS")
            print("=" * 70)
            if not df_sorted['auc'].isna().all():
                print(f"  Best AUC:    {df_sorted['auc'].max():.4f}  ({df_sorted.iloc[0]['model']})")
                print(f"  Worst AUC:   {df_sorted['auc'].min():.4f}  ({df_sorted.iloc[-1]['model']})")
                print(f"  Mean AUC:    {df_sorted['auc'].mean():.4f}")
                print(f"  Median AUC:  {df_sorted['auc'].median():.4f}")
                print(f"  Std Dev:     {df_sorted['auc'].std():.4f}")
        
        # ==================== ç”Ÿæˆå¯è§†åŒ– ====================
        viz_dir = create_visualizations(df, TEST_DATASET, timestamp)
        
        # ==================== æ‰“å°å®Œæ•´ç»“æœè¡¨æ ¼ ====================
        print("\n" + "=" * 70)
        print("COMPLETE RESULTS TABLE")
        print("=" * 70)
        print(df.to_string(index=False))
        
        # ==================== æœ€ç»ˆæ€»ç»“ ====================
        print("\n" + "=" * 70)
        print("ğŸ“ OUTPUT FILES")
        print("=" * 70)
        print(f"  CSV Results:         {output_file}")
        print(f"  Visualizations:      {viz_dir}/")
        print(f"    - 01_auc_comparison.png")
        print(f"    - 02_multi_metric_comparison.png")
        print(f"    - 03_ranking_visualization.png")
        print(f"    - 04_radar_chart_top5.png")
        print(f"    - 05_performance_heatmap.png")
    
    print("\n" + "=" * 70)
    print("âœ… Batch Testing Complete!")
    print("=" * 70)


if __name__ == '__main__':
    main()
